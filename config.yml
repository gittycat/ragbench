# Model Configuration
# This file defines all model settings for the RAG system.
# API keys are stored separately in secrets/.env

# ============================================================================
# Model Definitions - Define all available models here
# ============================================================================
models:
  inference:
    # Ollama models (local inference)
    gemma3-4b:
      provider: ollama
      model: gemma3:4b
      base_url: http://host.docker.internal:11434
      timeout: 120
      keep_alive: 10m  # Ollama-only: keep model in memory (10m, -1=forever, 0=unload immediately)

    llama3-8b:
      provider: ollama
      model: llama3:8b
      base_url: http://host.docker.internal:11434
      timeout: 120
      keep_alive: 10m

    # OpenAI
    gpt5-mini:
      provider: openai
      model: gpt-5-mini # good quality, much cheaper; great for most production RAG where retrieval is solid
      base_url: https://api.openai.com/v1
      timeout: 120
      requires_api_key: true

    # Anthropic models (cloud API)
    claude-sonnet:
      provider: anthropic
      model: claude-sonnet-4-20250514
      base_url: https://api.anthropic.com
      timeout: 120
      requires_api_key: true

    claude-opus:
      provider: anthropic
      model: claude-opus-4-6
      base_url: https://api.anthropic.com
      timeout: 120
      requires_api_key: true

    # Google models (cloud API)
    gemini-pro:
      provider: google
      model: gemini-pro
      timeout: 120
      requires_api_key: true

    # DeepSeek models (cloud API)
    deepseek-chat:
      provider: deepseek
      model: deepseek-chat
      timeout: 120
      requires_api_key: true

    # Moonshot models (cloud API)
    moonshot-v1:
      provider: moonshot
      model: moonshot-v1-8k
      base_url: https://api.moonshot.cn/v1
      timeout: 120
      requires_api_key: true

  embedding:
    # Ollama embeddings (local)
    nomic-embed:
      provider: ollama
      model: nomic-embed-text:latest
      base_url: http://host.docker.internal:11434

    # OpenAI embeddings (cloud API)
    openai-ada:
      provider: openai
      model: text-embedding-ada-002
      base_url: https://api.openai.com/v1
      requires_api_key: true

    openai-3-small:
      provider: openai
      model: text-embedding-3-small
      base_url: https://api.openai.com/v1
      requires_api_key: true

    openai-3-large:
      provider: openai
      model: text-embedding-3-large
      base_url: https://api.openai.com/v1
      requires_api_key: true

  # Rule of thumb: use a grader at least as capable as your answer (inference) model when you 
  # care about subtle errors or faithfulness; otherwise use a cheaper mini/nano model for scale.
  eval:
    # Anthropic models for evaluation (cloud API)
    claude-sonnet:
      provider: anthropic
      model: claude-sonnet-4-20250514
      requires_api_key: true

    claude-opus:
      provider: anthropic
      model: claude-opus-4-5-20251101
      requires_api_key: true

    # OpenAI models for evaluation (cloud API)
    gpt5-mini:
      provider: openai
      model: gpt-5-mini
      requires_api_key: true

  reranker:
    # Lightweight reranker (default)
    minilm-l6:
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      top_n: 5

    # More powerful rerankers
    bge-reranker-large:
      model: BAAI/bge-reranker-large
      top_n: 5

    bge-reranker-base:
      model: BAAI/bge-reranker-base
      top_n: 5

# ============================================================================
# Active Model Selection - Specify which models to use
# ============================================================================
active:
  inference: gpt5-mini
  embedding: nomic-embed
  eval: gpt5-mini
  reranker: minilm-l6

# ============================================================================
# Evaluation Settings (non-model-specific)
# ============================================================================
eval:
  # Citation scope for evaluation:
  # - retrieved: treat all retrieved chunks as citations
  # - explicit: only use explicitly cited chunks (if provided by the server)
  citation_scope: retrieved

  # Citation format for explicit citations
  # - numeric: uses [1], [2] style references mapped to source order
  citation_format: numeric

  # Abstention phrases used to detect "no answer" responses
  abstention_phrases:
    - "I don't have enough information to answer this question."
    - "I do not have enough information to answer this question."
    - "I don't have enough information to answer the question."
    - "I do not have enough information to answer the question."
    - "Not enough information to answer."
    - "Insufficient information to answer."

# ============================================================================
# Reranker Settings (non-model-specific)
# ============================================================================
reranker:
  enabled: true

# ============================================================================
# Retrieval Settings
# ============================================================================
retrieval:
  top_k: 10
  enable_hybrid_search: true    # BM25 + Vector search with RRF fusion
  rrf_k: 60                      # Reciprocal Rank Fusion parameter
  enable_contextual_retrieval: false  # Anthropic contextual retrieval (slower)

# ============================================================================
# Database Connection Pooling
# ============================================================================
database:
  # PostgreSQL server-side max_connections (applied via docker-compose command)
  max_connections: 200

  # Application-side connection pool (per service: rag-server, pgmq-worker)
  # Total connections per service = pool_size + max_overflow
  pool_size: 10         # Persistent connections kept open
  max_overflow: 20      # Additional connections allowed under burst load
  pool_pre_ping: true   # Verify connections before use (prevents stale connections)
  pool_recycle: 3600    # Recycle connections after 1 hour (seconds)

# ============================================================================
# Prompts - RAG pipeline prompts
# ============================================================================
prompts:
  # System-level instructions for LLM behavior
  system: |
    You are a professional assistant providing accurate answers based on document context.
    Be direct and concise. Avoid conversational fillers like 'Let me explain', 'Okay', 'Well', or 'Sure'.
    Start responses immediately with the answer.
    Use bullet points for lists when appropriate.

  # Context instructions for using retrieved documents
  # Placeholders: {context_str}, {chat_history}, {citation_instructions}
  context: |
    Context from retrieved documents:
    {context_str}

    Instructions:
    - Answer using ONLY the context provided above
    - If the context does not contain sufficient information, respond: "I don't have enough information to answer this question."
    - Never use prior knowledge or make assumptions beyond what is explicitly stated
    - Be specific and cite details from the context when relevant
    - Use citations consistently when referencing facts{citation_instructions}
    - Previous conversation context is available for reference

    Provide a direct, accurate answer based on the context:

  # Citation instructions (appended to context prompt when citations enabled)
  citation_instructions:
    numeric: |2

      - Add numeric citations in square brackets like [1], [2] that map to the order of context chunks provided above.

  # Question condensation prompt (null = use LlamaIndex default)
  # Default works well: "Given a conversation (between Human and Assistant) and a follow up
  # message from Human, rewrite the message to be a standalone question that captures all relevant context."
  condense: null

  # Contextual prefix generation for chunks (Anthropic method)
  # Placeholders: {document_name}, {document_type}, {chunk_preview}
  contextual_prefix: |
    Document: {document_name} ({document_type})

    Chunk content:
    {chunk_preview}

    Provide a concise 1-2 sentence context for this chunk, explaining what document it's from and what topic it discusses.
    Format: "This section from [document/topic] discusses [specific topic/concept]."

    Context (1-2 sentences only):
